# unified_parser.py
import asyncio
import aiohttp
import json
import sys
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import argparse
import logging
from concurrent.futures import ThreadPoolExecutor

// –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class MobilePlan:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–±–∏–ª—å–Ω–æ–≥–æ —Ç–∞—Ä–∏—Ñ–∞"""
    name: str
    operator: str
    price: str
    data: str
    calls: str = ""
    sms: str = ""
    validity: str = ""
    additional_info: str = ""
    source_url: str = ""

@dataclass
class SiteAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞"""
    name: str
    url: str
    status: str
    has_cookie_banner: bool
    cookie_selectors: List[str]
    requires_js: bool
    plan_containers: List[Dict[str, Any]]
    optimal_selectors: Dict[str, str]

class UnifiedNorwayMobileParser:
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ—Ä–≤–µ–∂—Å–∫–∏—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö —Ç–∞—Ä–∏—Ñ–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.plans: List[MobilePlan] = []
        self.site_analysis: Dict[str, SiteAnalysis] = {}
        
        // –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
        self.operators_config = {
            'telia': {
                'url': 'https://www.telia.no/privat/mobil/abonnement',
                'name': 'Telia',
                'fallback_selectors': {
                    'plan_cards': '.product-card, .plan-card, [data-testid*="plan"]',
                    'plan_name': 'h3, .plan-name, .product-title',
                    'price': '.price, .monthly-price, [data-testid*="price"]',
                    'data': '.data-amount, .gb-amount, [data-testid*="data"]'
                }
            },
            'telenor': {
                'url': 'https://www.telenor.no/privat/mobil/abonnement',
                'name': 'Telenor', 
                'fallback_selectors': {
                    'plan_cards': '.product-card, .plan-item, [data-cy*="plan"]',
                    'plan_name': 'h3, .plan-title, .product-name',
                    'price': '.price, .amount, [data-cy*="price"]',
                    'data': '.data-text, .gb-text, [data-cy*="data"]'
                }
            },
            'ice': {
                'url': 'https://www.ice.no/mobil/abonnement',
                'name': 'Ice',
                'fallback_selectors': {
                    'plan_cards': '.plan-card, .product-item',
                    'plan_name': 'h3, .plan-name, .title',
                    'price': '.price, .cost, .monthly-fee',
                    'data': '.data-limit, .gb-limit'
                }
            },
            'mycall': {
                'url': 'https://mycall.no',
                'name': 'MyCall',
                'fallback_selectors': {
                    'plan_cards': '.plan-box, .offer-card',
                    'plan_name': 'h3, .plan-title',
                    'price': '.price, .monthly-price',
                    'data': '.data-amount, .gb-info'
                }
            }
        }
        
        // Cookie-–±–∞–Ω–Ω–µ—Ä —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        self.cookie_selectors = {
            'accept_patterns': [
                '[data-testid*="accept"]', '[data-cy*="accept"]',
                '.cookie-accept', '.accept-cookies', '.gdpr-accept',
                'button[class*="accept"]', 'button[id*="accept"]',
                'button:-soup-contains("Godta")', 'button:-soup-contains("Accept")',
                'button:-soup-contains("Aksepter")', '[aria-label*="accept"]'
            ],
            'banner_patterns': [
                '.cookie-banner', '.gdpr-banner', '.consent-banner',
                '#cookie-notice', '[class*="cookie"]', '[role="dialog"]'
            ]
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5,no;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive'
        }

    async def __aenter__(self):
        connector = aiohttp.TCPConnector(
            limit=10, limit_per_host=3, ttl_dns_cache=300, ssl=False
        )
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=timeout,
            cookie_jar=aiohttp.CookieJar(unsafe=True)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        return re.sub(r'\s+', ' ', text.strip()) if text else ""

    def _extract_price(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        
        price_patterns = [
            r'(\d+(?:[,\.]\d+)?)\s*kr\b',
            r'(\d+(?:[,\.]\d+)?)\s*NOK\b', 
            r'\bkr\s*(\d+(?:[,\.]\d+)?)',
            r'(\d+(?:[,\.]\d+)?)\s*,-'
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                price = match.group(1).replace(',', '.')
                return f"{price} kr"
        
        return self._clean_text(text)

    def _extract_data_amount(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if not text:
            return ""
        
        text_lower = text.lower()
        
        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ–∑–ª–∏–º–∏—Ç
        unlimited_keywords = ['ubegrenset', 'unlimited', 'fri data', 'uten grense']
        if any(keyword in text_lower for keyword in unlimited_keywords):
            return "Unlimited"
        
        data_patterns = [
            r'(\d+(?:[,\.]\d+)?)\s*GB\b',
            r'(\d+(?:[,\.]\d+)?)\s*TB\b',
            r'(\d+(?:[,\.]\d+)?)\s*MB\b',
            r'(\d+)\s*giga\b'
        ]
        
        for pattern in data_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                amount = match.group(1).replace(',', '.')
                unit = 'GB' if 'giga' in pattern else match.group(0)[-2:]
                return f"{amount} {unit}"
        
        return self._clean_text(text)

    async def _fetch_with_retry(self, url: str, max_retries: int = 3) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(max_retries):
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                async with self.session.get(url, allow_redirects=True) as response:
                    if response.status == 200:
                        content = await response.text()
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        return content
                    else:
                        logger.warning(f"HTTP {response.status}")
            except asyncio.TimeoutError:
                logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
            
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        return None

    def _detect_cookie_elements(self, soup: BeautifulSoup) -> Dict[str, List[str]]:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cookie —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        cookie_elements = {
            'accept_buttons': [],
            'banners': []
        }
        
        // –ü–æ–∏—Å–∫ –∫–Ω–æ–ø–æ–∫ –ø—Ä–∏–Ω—è—Ç–∏—è
        for pattern in self.cookie_selectors['accept_patterns']:
            try:
                elements = soup.select(pattern)
                for elem in elements:
                    text = elem.get_text().strip().lower()
                    if any(word in text for word in ['godta', 'accept', 'ok', 'aksepter']):
                        cookie_elements['accept_buttons'].append(pattern)
                        break
            except:
                continue
        
        // –ü–æ–∏—Å–∫ –±–∞–Ω–Ω–µ—Ä–æ–≤
        for pattern in self.cookie_selectors['banner_patterns']:
            try:
                if soup.select(pattern):
                    cookie_elements['banners'].append(pattern)
            except:
                continue
        
        return cookie_elements

    def _analyze_plan_containers(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ —Å –ø–ª–∞–Ω–∞–º–∏"""
        containers = []
        
        container_patterns = [
            '.plan', '.product', '.card', '.subscription', '.abonnement',
            '.offer', '.package', '.tariff', '[class*="plan"]',
            '[class*="product"]', '[class*="subscription"]'
        ]
        
        for pattern in container_patterns:
            try:
                elements = soup.select(pattern)
                if len(elements) >= 2:  // –ú–∏–Ω–∏–º—É–º 2 —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
                    sample_text = elements[0].get_text()[:100] if elements else ""
                    containers.append({
                        'selector': pattern,
                        'count': len(elements),
                        'sample_text': sample_text,
                        'confidence': self._calculate_container_confidence(elements)
                    })
            except:
                continue
        
        // –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        return sorted(containers, key=lambda x: x['confidence'], reverse=True)

    def _calculate_container_confidence(self, elements: List) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ –ø–ª–∞–Ω–æ–≤"""
        if not elements:
            return 0.0
        
        confidence = 0.0
        keywords = ['kr', 'gb', 'plan', 'mobil', 'abonnement', 'm√•ned']
        
        for elem in elements[:3]:  // –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞
            text = elem.get_text().lower()
            keyword_matches = sum(1 for keyword in keywords if keyword in text)
            confidence += keyword_matches / len(keywords)
        
        return confidence / min(len(elements), 3)

    def _optimize_selectors(self, containers: List[Dict[str, Any]], 
                          fallback_selectors: Dict[str, str]) -> Dict[str, str]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        optimized = fallback_selectors.copy()
        
        if containers:
            best_container = containers[0]
            if best_container['confidence'] > 0.5:
                optimized['plan_cards'] = best_container['selector']
        
        return optimized

    async def analyze_site_structure(self, operator_key: str) -> SiteAnalysis:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞"""
        config = self.operators_config[operator_key]
        
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã {config['name']}")
        
        html = await self._fetch_with_retry(config['url'])
        if not html:
            return SiteAnalysis(
                name=config['name'], url=config['url'], status='failed',
                has_cookie_banner=False, cookie_selectors=[],
                requires_js=False, plan_containers=[],
                optimal_selectors=config['fallback_selectors']
            )
        
        soup = BeautifulSoup(html, 'html.parser')
        
        // –ê–Ω–∞–ª–∏–∑ cookie —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        cookie_elements = self._detect_cookie_elements(soup)
        has_cookie_banner = bool(cookie_elements['accept_buttons'] or cookie_elements['banners'])
        
        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JavaScript
        requires_js = any(indicator in html.lower() for indicator in [
            'document.getelementbyid', 'react', 'vue', 'angular', 'loading...'
        ])
        
        // –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –ø–ª–∞–Ω–æ–≤
        plan_containers = self._analyze_plan_containers(soup)
        
        // –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
        optimal_selectors = self._optimize_selectors(plan_containers, config['fallback_selectors'])
        
        analysis = SiteAnalysis(
            name=config['name'],
            url=config['url'],
            status='success',
            has_cookie_banner=has_cookie_banner,
            cookie_selectors=cookie_elements['accept_buttons'],
            requires_js=requires_js,
            plan_containers=plan_containers,
            optimal_selectors=optimal_selectors
        )
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ {config['name']} –∑–∞–≤–µ—Ä—à–µ–Ω")
        return analysis

    def _parse_plans_from_html(self, html: str, analysis: SiteAnalysis) -> List[MobilePlan]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–ª–∞–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        plans = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            selectors = analysis.optimal_selectors
            
            // –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø–ª–∞–Ω–æ–≤
            plan_elements = []
            for selector in selectors['plan_cards'].split(', '):
                elements = soup.select(selector.strip())
                plan_elements.extend(elements)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(plan_elements)} –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è {analysis.name}")
            
            for element in plan_elements:
                try:
                    // –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–ª–∞–Ω–∞
                    name = self._extract_from_element(element, selectors['plan_name'])
                    price = self._extract_price(self._extract_from_element(element, selectors['price']))
                    data = self._extract_data_amount(self._extract_from_element(element, selectors['data']))
                    
                    // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    additional_info = self._extract_additional_info(element)
                    
                    if name or price or data:
                        plan = MobilePlan(
                            name=name or "–ù–µ —É–∫–∞–∑–∞–Ω–æ",
                            operator=analysis.name,
                            price=price or "–ù–µ —É–∫–∞–∑–∞–Ω–æ", 
                            data=data or "–ù–µ —É–∫–∞–∑–∞–Ω–æ",
                            additional_info=additional_info,
                            source_url=analysis.url
                        )
                        plans.append(plan)
                        logger.info(f"üì± –ü–ª–∞–Ω: {plan.name} - {plan.price}")
                        
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML –¥–ª—è {analysis.name}: {e}")
        
        return plans

    def _extract_from_element(self, element, selectors: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º"""
        for selector in selectors.split(', '):
            try:
                elem = element.select_one(selector.strip())
                if elem:
                    return self._clean_text(elem.get_text())
            except:
                continue
        return ""

    def _extract_additional_info(self, element) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–ª–∞–Ω–µ"""
        info_parts = []
        
        // –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–≤–æ–Ω–∫–∞—Ö –∏ SMS
        text = element.get_text().lower()
        
        if any(word in text for word in ['ubegrenset samtaler', 'fri samtaler', 'unlimited calls']):
            info_parts.append("–ë–µ–∑–ª–∏–º–∏—Ç–Ω—ã–µ –∑–≤–æ–Ω–∫–∏")
        
        if any(word in text for word in ['ubegrenset sms', 'fri sms', 'unlimited sms']):
            info_parts.append("–ë–µ–∑–ª–∏–º–∏—Ç–Ω—ã–µ SMS")
        
        if any(word in text for word in ['5g', '5g-nett', '5g network']):
            info_parts.append("5G –ø–æ–¥–¥–µ—Ä–∂–∫–∞")
        
        return "; ".join(info_parts)

    async def parse_operator(self, operator_key: str) -> Tuple[List[MobilePlan], SiteAnalysis]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞—Ä–∏—Ñ–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞"""
        logger.info(f"üöÄ –ü–∞—Ä—Å–∏–Ω–≥ {operator_key}")
        
        // –°–Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        analysis = await self.analyze_site_structure(operator_key)
        self.site_analysis[operator_key] = analysis
        
        if analysis.status != 'success':
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å {operator_key}")
            return [], analysis
        
        // –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        html = await self._fetch_with_retry(analysis.url)
        if not html:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {operator_key}")
            return [], analysis
        
        // –ü–∞—Ä—Å–∏–º –ø–ª–∞–Ω—ã
        plans = self._parse_plans_from_html(html, analysis)
        
        logger.info(f"‚úÖ {operator_key}: –Ω–∞–π–¥–µ–Ω–æ {len(plans)} –ø–ª–∞–Ω–æ–≤")
        return plans, analysis

    async def parse_all_operators(self) -> List[MobilePlan]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥")
        
        tasks = [self.parse_operator(key) for key in self.operators_config.keys()]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_plans = []
        for i, result in enumerate(results):
            operator_key = list(self.operators_config.keys())[i]
            
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ {operator_key}: {result}")
            else:
                plans, analysis = result
                all_plans.extend(plans)
        
        self.plans = all_plans
        logger.info(f"üéâ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_plans)} –ø–ª–∞–Ω–æ–≤")
        return all_plans

    def save_results(self, filename: str = 'norway_mobile_plans.json'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            // –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º
            operator_stats = {}
            for plan in self.plans:
                if plan.operator not in operator_stats:
                    operator_stats[plan.operator] = 0
                operator_stats[plan.operator] += 1
            
            data = {
                'metadata': {
                    'total_plans': len(self.plans),
                    'operators_found': list(operator_stats.keys()),
                    'operator_stats': operator_stats,
                    'parsing_analysis': {k: asdict(v) for k, v in self.site_analysis.items()}
                },
                'plans': [asdict(plan) for plan in self.plans]
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

    def print_analysis_summary(self):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print(f"\n{'='*80}")
        print("üìä –°–í–û–î–ö–ê –ê–ù–ê–õ–ò–ó–ê –ò –ü–ê–†–°–ò–ù–ì–ê")
        print(f"{'='*80}")
        
        // –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–æ–≤
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –°–ê–ô–¢–û–í:")
        for key, analysis in self.site_analysis.items():
            print(f"\nüì± {analysis.name}:")
            print(f"   –°—Ç–∞—Ç—É—Å: {analysis.status}")
            print(f"   Cookie –±–∞–Ω–Ω–µ—Ä: {'–î–∞' if analysis.has_cookie_banner else '–ù–µ—Ç'}")
            print(f"   –¢—Ä–µ–±—É–µ—Ç JS: {'–î–∞' if analysis.requires_js else '–ù–µ—Ç'}")
            
            if analysis.plan_containers:
                best = analysis.plan_containers[0]
                print(f"   –õ—É—á—à–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä: {best['selector']} ({best['count']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best['confidence']:.2f})")
        
        // –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê:")
        print(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø–ª–∞–Ω–æ–≤: {len(self.plans)}")
        
        if not self.plans:
            print("‚ùå –ü–ª–∞–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        // –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º
        by_operator = {}
        for plan in self.plans:
            if plan.operator not in by_operator:
                by_operator[plan.operator] = []
            by_operator[plan.operator].append(plan)
        
        for operator, plans in by_operator.items():
            print(f"\nüì± {operator}: {len(plans)} –ø–ª–∞–Ω–æ–≤")
            for plan in plans[:3]:
                print(f"   ‚Ä¢ {plan.name} - {plan.price} - {plan.data}")
                if plan.additional_info:
                    print(f"     –î–æ–ø. –∏–Ω—Ñ–æ: {plan.additional_info}")
            
            if len(plans) > 3:
                print(f"   ... –∏ –µ—â–µ {len(plans) - 3} –ø–ª–∞–Ω–æ–≤")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä –Ω–æ—Ä–≤–µ–∂—Å–∫–∏—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö —Ç–∞—Ä–∏—Ñ–æ–≤')
    parser.add_argument('--operator', choices=['telia', 'telenor', 'ice', 'mycall'],
                       help='–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞')
    parser.add_argument('--analyze-only', action='store_true',
                       help='–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞')
    parser.add_argument('--output', '-o', default='norway_mobile_plans.json',
                       help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    parser.add_argument('--silent', '-s', action='store_true',
                       help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥')
    
    args = parser.parse_args()
    
    if args.silent:
        logging.getLogger().setLevel(logging.WARNING)
    
    try:
        async with UnifiedNorwayMobileParser() as parser_instance:
            if args.analyze_only:
                // –¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                for key in parser_instance.operators_config.keys():
                    if not args.operator or args.operator == key:
                        await parser_instance.analyze_site_structure(key)
            else:
                // –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
                if args.operator:
                    await parser_instance.parse_operator(args.operator)
                else:
                    await parser_instance.parse_all_operators()
                
                parser_instance.save_results(args.output)
            
            if not args.silent:
                parser_instance.print_analysis_summary()
                
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())