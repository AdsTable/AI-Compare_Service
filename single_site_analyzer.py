# single_site_analyzer.py
import requests
import time
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import ssl
import socket
from collections import defaultdict
import re

class WebsiteDiagnostic:
    def __init__(self, base_url, max_pages=50):
        self.base_url = base_url.rstrip('/')
        self.max_pages = max_pages
        self.visited_urls = set()
        self.broken_links = []
        self.slow_pages = []
        self.seo_issues = []
        self.security_issues = []
        self.performance_data = {}
        
    def check_url_accessibility(self, url, timeout=10):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ URL —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞"""
        try:
            start_time = time.time()
            response = requests.get(url, timeout=timeout, allow_redirects=True)
            response_time = time.time() - start_time
            
            return {
                'accessible': True,
                'status_code': response.status_code,
                'response_time': response_time,
                'final_url': response.url,
                'content': response.text if response.status_code == 200 else None
            }
        except requests.exceptions.RequestException as e:
            return {
                'accessible': False,
                'error': str(e),
                'response_time': None
            }
    
    def extract_links(self, html_content, base_url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = set()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–≥–æ–≤ <a>
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            if self.is_internal_link(full_url):
                links.add(full_url)
        
        return links
    
    def is_internal_link(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π"""
        return urlparse(url).netloc == urlparse(self.base_url).netloc
    
    def analyze_seo(self, html_content, url):
        """–ê–Ω–∞–ª–∏–∑ SEO –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html_content, 'html.parser')
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ title
        title = soup.find('title')
        if not title or not title.text.strip():
            issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç title –Ω–∞ {url}")
        elif len(title.text) > 60:
            issues.append(f"Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –Ω–∞ {url} ({len(title.text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc:
            issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description –Ω–∞ {url}")
        elif len(meta_desc.get('content', '')) > 160:
            issues.append(f"Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –Ω–∞ {url}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ H1
        h1_tags = soup.find_all('h1')
        if not h1_tags:
            issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1 –Ω–∞ {url}")
        elif len(h1_tags) > 1:
            issues.append(f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ H1 –Ω–∞ {url}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ alt –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        images = soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        if images_without_alt:
            issues.append(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ alt –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –Ω–∞ {url}: {len(images_without_alt)}")
        
        return issues
    
    def check_security(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ HTTPS
        if not url.startswith('https://'):
            issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç HTTPS: {url}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞
        try:
            hostname = urlparse(url).hostname
            context = ssl.create_default_context()
            with socket.create_connection((hostname, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
                    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞
        except Exception as e:
            if url.startswith('https://'):
                issues.append(f"–ü—Ä–æ–±–ª–µ–º—ã —Å SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–º –¥–ª—è {url}: {str(e)}")
        
        return issues
    
    def crawl_website(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞"""
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É —Å–∞–π—Ç–∞: {self.base_url}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {self.max_pages}")
        print("-" * 60)
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        urls_to_visit = [self.base_url]
        
        while urls_to_visit and len(self.visited_urls) < self.max_pages:
            current_url = urls_to_visit.pop(0)
            if current_url in self.visited_urls:
                continue
            
            print(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º: {current_url}")
            self.visited_urls.add(current_url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            result = self.check_url_accessibility(current_url)
            
            if not result['accessible']:
                self.broken_links.append({
                    'url': current_url,
                    'error': result['error']
                })
                continue
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            self.performance_data[current_url] = {
                'response_time': result['response_time'],
                'status_code': result['status_code']
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if result['response_time'] > 3.0:
                self.slow_pages.append({
                    'url': current_url,
                    'response_time': result['response_time']
                })
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º SEO
            if result['content']:
                seo_issues = self.analyze_seo(result['content'], current_url)
                self.seo_issues.extend(seo_issues)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
                new_links = self.extract_links(result['content'], current_url)
                for link in new_links:
                    if link not in self.visited_urls and link not in urls_to_visit:
                        urls_to_visit.append(link)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
            security_issues = self.check_security(current_url)
            self.security_issues.extend(security_issues)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(0.5)
    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\n" + "=" * 60)
        print("           –û–¢–ß–ï–¢ –û –î–ò–ê–ì–ù–û–°–¢–ò–ö–ï –°–ê–ô–¢–ê")
        print("=" * 60)
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(self.visited_urls)}")
        print(f"–°–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(self.broken_links)}")
        print(f"–ú–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {len(self.slow_pages)}")
        print(f"SEO –ø—Ä–æ–±–ª–µ–º: {len(self.seo_issues)}")
        print(f"–ü—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {len(self.security_issues)}")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if self.performance_data:
            avg_response_time = sum(data['response_time'] for data in self.performance_data.values()) / len(self.performance_data)
            print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {avg_response_time:.2f} —Å–µ–∫")
        
        print("-" * 60)
        
        # –°–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if self.broken_links:
            print("\nüî¥ –°–õ–û–ú–ê–ù–ù–´–ï –°–°–´–õ–ö–ò:")
            for link in self.broken_links[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  ‚Ä¢ {link['url']}")
                print(f"    –û—à–∏–±–∫–∞: {link['error']}")
        else:
            print("\n‚úÖ –°–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if self.slow_pages:
            print(f"\nüêå –ú–ï–î–õ–ï–ù–ù–´–ï –°–¢–†–ê–ù–ò–¶–´ (–≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ > 3 —Å–µ–∫):")
            sorted_slow = sorted(self.slow_pages, key=lambda x: x['response_time'], reverse=True)
            for page in sorted_slow[:10]:
                print(f"  ‚Ä¢ {page['url']} ({page['response_time']:.2f} —Å–µ–∫)")
        else:
            print("\n‚ö° –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –±—ã—Å—Ç—Ä–æ")
        
        # SEO –ø—Ä–æ–±–ª–µ–º—ã
        if self.seo_issues:
            print(f"\nüîç SEO –ü–†–û–ë–õ–ï–ú–´:")
            issue_counts = defaultdict(int)
            for issue in self.seo_issues:
                issue_type = issue.split(' –Ω–∞ ')[0] if ' –Ω–∞ ' in issue else issue
                issue_counts[issue_type] += 1
            
            for issue_type, count in issue_counts.items():
                print(f"  ‚Ä¢ {issue_type}: {count} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            print("\n–ü–µ—Ä–≤—ã–µ 10 SEO –ø—Ä–æ–±–ª–µ–º:")
            for issue in self.seo_issues[:10]:
                print(f"  - {issue}")
        else:
            print("\n‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö SEO –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –ü—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        if self.security_issues:
            print(f"\nüõ°Ô∏è –ü–†–û–ë–õ–ï–ú–´ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò:")
            for issue in self.security_issues[:10]:
                print(f"  ‚Ä¢ {issue}")
        else:
            print("\nüîí –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        print("-" * 60)
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        
        recommendations = []
        
        if self.broken_links:
            recommendations.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞")
        
        if self.slow_pages:
            recommendations.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
            recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CDN –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        if any("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç title" in issue for issue in self.seo_issues):
            recommendations.append("–î–æ–±–∞–≤—å—Ç–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ title –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        if any("meta description" in issue for issue in self.seo_issues):
            recommendations.append("–°–æ–∑–¥–∞–π—Ç–µ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ –Ω–∏—Ö")
        
        if any("alt –∞—Ç—Ä–∏–±—É—Ç–æ–≤" in issue for issue in self.seo_issues):
            recommendations.append("–î–æ–±–∞–≤—å—Ç–µ alt-–∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        if any("HTTPS" in issue for issue in self.security_issues):
            recommendations.append("–ü–µ—Ä–µ–≤–µ–¥–∏—Ç–µ —Å–∞–π—Ç –Ω–∞ HTTPS –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        
        if not recommendations:
            recommendations.append("–í–∞—à —Å–∞–π—Ç –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏! –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
        
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
        
        print("\n" + "=" * 60)
        print("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
        return {
            'total_pages': len(self.visited_urls),
            'broken_links': len(self.broken_links),
            'slow_pages': len(self.slow_pages),
            'seo_issues': len(self.seo_issues),
            'security_issues': len(self.security_issues),
            'avg_response_time': sum(data['response_time'] for data in self.performance_data.values()) / len(self.performance_data) if self.performance_data else 0
        }

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
def run_website_diagnostic(url, max_pages=50):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å–∞–π—Ç–∞"""
    try:
        diagnostic = WebsiteDiagnostic(url, max_pages)
        diagnostic.crawl_website()
        return diagnostic.generate_report()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ: {e}")
        return None

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ URL –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
    website_url = "https://adstable.com"
    
    print("üîç –£—Ç–∏–ª–∏—Ç–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞")
    print("=" * 60)
    
    # –ú–æ–∂–Ω–æ –∑–∞–ø—Ä–æ—Å–∏—Ç—å URL —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_url = input("–í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è AdsTable.com): ").strip()
    if user_url:
        website_url = user_url
    
    max_pages = input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50): ").strip()
    max_pages = int(max_pages) if max_pages.isdigit() else 50
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
    results = run_website_diagnostic(website_url, max_pages)
    
    if results:
        print(f"\nüìà –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
        print(f"–û–±—â–∏–π –±–∞–ª–ª —Å–∞–π—Ç–∞: {100 - (results['broken_links'] * 10 + results['slow_pages'] * 5 + results['seo_issues'] * 2)}/100")